---
layout: article
cover: /articles/2025/octubre/latenciaup.jpg
# cover: /articles/2025/agosto/margareth.jpeg
author:
  name: Ever Burga
description: Los microservicios y la nube tienen un costo oculto...


date: 2025-10-24T10:00:00.000Z
---

# El Antipatr√≥n de la Red Veloz

Hace un tiempo le√≠ el excelente libro Fundamentals of Software Architecture (2020).
Y me llam√≥ la atenci√≥n lo que los autores llaman **falacia de la latencia cero**.

Fui esc√©ptico... ¬øc√≥mo podr√≠a saltarme algo tan obvio?

<p align="center">
  <img src="/articles/2025/octubre/latenciaup.jpg" alt="Latencia vs">
</p>

## Mirando a las hojas del √°rbol
Uno de nuestros clientes pidi√≥ la modernizaci√≥n de sus aplicaciones web. 

La raz√≥n: **durante escenarios de alta demanda los servidores se caen**. 

La nube y su alta escalabilidad es una soluci√≥n v√°lida para este caso.
Es as√≠ que comenzamos una PoC desplegando uno de los componentes en la nube.

Y reventamos de solicitudes a la PoC para probar la escalabilidad y resiliencia de nuestra arquitectura. 

<p align="center">
	<img src="/articles/2025/octubre/alta_demanda_nube.png" alt="Alta demanda nube" title="resultados de las pruebas con locust">
	<em>resultados de las pruebas con locust</em>
</p>

Funcionaba con una mejora del 65% en tiempos de respuesta. Mucho mejor de lo que esperaba para la migraci√≥n de un solo componente. 


<p align="center">
	<img src="/articles/2025/octubre/bosque.jpg" alt="Alta demanda nube" title="bosque">
	<em>ahora pienso que est√°bamos mirando las hojas del √°rbol en lugar del bosque</em>
</p>


¬°Con esto √≠bamos a solucionar la demanda actual y adem√°s nos preparamos para 
el crecimiento esperado del negocio!

## Acercando la lupa

Al principio prioric√© las pruebas de estr√©s bajo alta demanda. 
Luego se me ocurri√≥ que si hubo mejora en los escenarios extremos
pues entonces la mejora tambi√©n fue absoluta. El hardware de nuestras instancias
en la nube era mejor, de todos modos. As√≠ tenemos otro argumento para agregar las bullets 
de nuestra presentaci√≥n. 

Y ah√≠ me revent√≥ en la cara. 

**30ms on premises** vs **600ms on cloud** en uno de los endpoints. 
Y situaci√≥n similar en los dem√°s.

Ups...

No pod√≠a salirse de mi control una PoC ya casi cerrada. Me puse a investigar la causa ra√≠z y record√© que 
nuestro componente en la nube, en EEUU, llama servicios aqu√≠ en Lima. A pesar de que todo se hace a trav√©s de 
Direct Connect, igual la latencia f√≠sica es real.

<p align="center">
	<img src="/articles/2025/octubre/diagramacomu.jpg" alt="diagramacomu">
	<em>mi educaci√≥n no es en teleco, pero a√∫n me da risa pensar que no lo pens√© antes</em>
</p>


## ¬øY luego?

Conversamos con el cliente que si bien 600ms no es para morirse, en el percentil 95% se
llega a valores de hasta 2 segundos. Y nadie aguanta eso en un login sin desesperarse. 
Menos en otros endpoints core.

Cambiamos el plan de despliegue de regi√≥n a Brasil. Aqu√≠ tenemos mejor latencia.

Y estamos en evaluaci√≥n sobre las siguientes etapas de la
modernizaci√≥n. Hay que definir el despliegue progresivo de los nuevos microservicios
de modo que la latencia no se convierta en un problema.

Nos gustar√≠a migrar todos los componentes de un tiro, pero nuestro 
cliente pierde dinero al momento al que hablamos por culpa de la falta de 
escalabilidad. 

**Los microservicios y la nube tienen un costo oculto de latencia**.
Y otros m√°s que se explican en el libro... que ahora tengo que repasar. üòÖ



## Referencias

Mark Richard & Neal Ford. 2020. Fundamentals of Software Architecture: An Engineering Approach. O'Reilly